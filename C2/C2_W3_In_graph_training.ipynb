{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "C2_W3_In-graph training.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "metric-approach"
      },
      "source": [
        "import numpy                      as np\n",
        "import tensorflow                 as tf\n",
        "import tensorflow_hub             as hub\n",
        "import tensorflow_datasets        as tfds"
      ],
      "id": "metric-approach",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UpFpF469sel"
      },
      "source": [
        "# Data"
      ],
      "id": "3UpFpF469sel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jdzx3PgIfFSl"
      },
      "source": [
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    'mnist',\n",
        "    split=['train', 'test'],\n",
        "    shuffle_files=True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")"
      ],
      "id": "Jdzx3PgIfFSl",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voBQ5ZNiAF4M"
      },
      "source": [
        "## Pre-porcess image"
      ],
      "id": "voBQ5ZNiAF4M"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YspcSk3fiNh"
      },
      "source": [
        "@tf.function\n",
        "\n",
        "def normalize(img, label):\n",
        "  return tf.cast(img, float)/255., label"
      ],
      "id": "4YspcSk3fiNh",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNtI1tlrgLFu"
      },
      "source": [
        "ds_train = ds_train.map(normalize, num_parallel_calls= tf.data.experimental.AUTOTUNE)\n",
        "ds_train = ds_train.shuffle(buffer_size=1000).batch(128)\n",
        "ds_train = ds_train.prefetch(1000)"
      ],
      "id": "aNtI1tlrgLFu",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsSUg2A2BURi"
      },
      "source": [
        "ds_test = ds_test.map(normalize, num_parallel_calls= tf.data.experimental.AUTOTUNE)\n",
        "ds_test = ds_test.shuffle(buffer_size=1000).batch(128)\n",
        "ds_test = ds_test.prefetch(1000)"
      ],
      "id": "tsSUg2A2BURi",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9lXfqphH8KG"
      },
      "source": [
        "## Model"
      ],
      "id": "u9lXfqphH8KG"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLtKNqCnLiMU",
        "outputId": "7e846d19-e92c-425a-ee46-cefb7e62b98d"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                          tf.keras.layers.Conv2D(64, 3, activation='relu', input_shape = (28,28,1)),\n",
        "                          tf.keras.layers.MaxPool2D(2),\n",
        "                          tf.keras.layers.Flatten(),\n",
        "                          tf.keras.layers.Dense(64, activation='relu'),\n",
        "                          tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "id": "xLtKNqCnLiMU",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 64)                692288    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 693,578\n",
            "Trainable params: 693,578\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B59rlpv2W3KX"
      },
      "source": [
        "## Optimizer, loss and accuracy"
      ],
      "id": "B59rlpv2W3KX"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQcecG3pF1g6"
      },
      "source": [
        "def set_optimizer():\n",
        "  return tf.optimizers.Adam()\n",
        "\n",
        "\n",
        "def set_losses():\n",
        "    train_loss = tf.losses.SparseCategoricalCrossentropy()\n",
        "    val_loss = tf.losses.SparseCategoricalCrossentropy() \n",
        "    return train_loss, val_loss\n",
        "\n",
        "\n",
        "def set_accuracy():\n",
        "  train_acc = tf.metrics.SparseCategoricalAccuracy()    \n",
        "  val_acc = tf.metrics.SparseCategoricalAccuracy() \n",
        "  return train_acc, val_acc"
      ],
      "id": "bQcecG3pF1g6",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i5fXclOLEtF"
      },
      "source": [
        "optimizer = set_optimizer()\n",
        "train_loss, val_loss = set_losses()\n",
        "train_acc , val_acc = set_accuracy()"
      ],
      "id": "2i5fXclOLEtF",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuhND5ajYvLP"
      },
      "source": [
        "## One training loop"
      ],
      "id": "GuhND5ajYvLP"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeVTRiVfYFcE"
      },
      "source": [
        "def one_epoch(model, optimizer, X, y, losses, accuracy):\n",
        "  with tf.GradientTape() as tape:\n",
        "    pred = model(X)\n",
        "    loss = train_loss(y, pred)\n",
        "\n",
        "  grad = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grad, model.trainable_variables))\n",
        "\n",
        "  train_acc(y, pred)\n",
        "\n",
        "  return loss"
      ],
      "id": "KeVTRiVfYFcE",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQtn8x8aaZ5d"
      },
      "source": [
        "## full model"
      ],
      "id": "TQtn8x8aaZ5d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5IL0zTlaO01"
      },
      "source": [
        "@tf.function\n",
        "\n",
        "def train(model, device, num_epochs, optimizer, train_data, train_loss, train_acc, val_data, val_loss, val_acc):\n",
        "\n",
        "    step = 0\n",
        "    loss = 0.\n",
        "\n",
        "    for i in range(num_epochs):\n",
        "        for x, y in train_data:\n",
        "            step +=1\n",
        "            with tf.device(device_name=device):\n",
        "              loss = one_epoch(model, optimizer, x, y, train_loss, train_acc)\n",
        "\n",
        "# print loss after each batch (128)\n",
        "            tf.print('step ', step, \n",
        "                  ': loss' , loss,\n",
        "                \": acc\", train_acc.result())\n",
        "        \n",
        "        \n",
        "        with tf.device(device_name=device):\n",
        "            for x, y in val_data:\n",
        "                pred = model(x)\n",
        "                loss = val_loss(y, pred)\n",
        "                val_acc(y, pred)\n",
        "\n",
        "        tf.print('val_loss' , loss,\n",
        "                        \": val_ acc\", val_acc.result())\n",
        "          \n",
        "\n",
        "\n"
      ],
      "id": "q5IL0zTlaO01",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31eYRkkFgeEv"
      },
      "source": [
        "# this code uses the GPU if available, otherwise uses a CPU\n",
        "device = '/gpu:0' if tf.test.is_gpu_available() else '/cpu:0'\n",
        "EPOCHS = 2\n"
      ],
      "id": "31eYRkkFgeEv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tz8E0etlbhsI",
        "outputId": "36fe03ef-9edf-4014-d172-8ff5a5b27768"
      },
      "source": [
        "train(model, device, 2, optimizer, ds_train, train_loss, train_acc, ds_test, val_loss, val_acc)"
      ],
      "id": "tz8E0etlbhsI",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "step  1 : loss 2.30282164 : acc 0.078125\n",
            "step  2 : loss 2.16940808 : acc 0.15234375\n",
            "step  3 : loss 2.04881334 : acc 0.239583328\n",
            "step  4 : loss 1.90542412 : acc 0.3046875\n",
            "step  5 : loss 1.80432487 : acc 0.364062488\n",
            "step  6 : loss 1.56639624 : acc 0.41015625\n",
            "step  7 : loss 1.46044123 : acc 0.446428567\n",
            "step  8 : loss 1.42975104 : acc 0.469726562\n",
            "step  9 : loss 1.19611955 : acc 0.502604187\n",
            "step  10 : loss 1.06518841 : acc 0.53125\n",
            "step  11 : loss 1.0008018 : acc 0.552556813\n",
            "step  12 : loss 0.984683394 : acc 0.571614563\n",
            "step  13 : loss 0.853455 : acc 0.588942289\n",
            "step  14 : loss 0.772534311 : acc 0.604910731\n",
            "step  15 : loss 0.832049251 : acc 0.614583313\n",
            "step  16 : loss 0.75705874 : acc 0.624023438\n",
            "step  17 : loss 0.768032432 : acc 0.632352948\n",
            "step  18 : loss 0.63091749 : acc 0.642361104\n",
            "step  19 : loss 0.601264417 : acc 0.650082231\n",
            "step  20 : loss 0.568347573 : acc 0.660937488\n",
            "step  21 : loss 0.509435 : acc 0.669642866\n",
            "step  22 : loss 0.529275298 : acc 0.677201688\n",
            "step  23 : loss 0.449446619 : acc 0.685122311\n",
            "step  24 : loss 0.590043902 : acc 0.69140625\n",
            "step  25 : loss 0.586981177 : acc 0.695\n",
            "step  26 : loss 0.489117891 : acc 0.700721145\n",
            "step  27 : loss 0.624418378 : acc 0.704571784\n",
            "step  28 : loss 0.560833097 : acc 0.708426356\n",
            "step  29 : loss 0.471699297 : acc 0.713092685\n",
            "step  30 : loss 0.439331472 : acc 0.717968762\n",
            "step  31 : loss 0.321788222 : acc 0.724042356\n",
            "step  32 : loss 0.49756223 : acc 0.727539062\n",
            "step  33 : loss 0.25615871 : acc 0.733428\n",
            "step  34 : loss 0.488341093 : acc 0.737362146\n",
            "step  35 : loss 0.300453246 : acc 0.741964281\n",
            "step  36 : loss 0.398845434 : acc 0.74609375\n",
            "step  37 : loss 0.584466577 : acc 0.748310804\n",
            "step  38 : loss 0.415082216 : acc 0.751439154\n",
            "step  39 : loss 0.632343531 : acc 0.753004789\n",
            "step  40 : loss 0.279497147 : acc 0.756445289\n",
            "step  41 : loss 0.42735517 : acc 0.758765221\n",
            "step  42 : loss 0.412353218 : acc 0.761346698\n",
            "step  43 : loss 0.522598922 : acc 0.763808131\n",
            "step  44 : loss 0.335449547 : acc 0.767223\n",
            "step  45 : loss 0.429487705 : acc 0.77013886\n",
            "step  46 : loss 0.328512609 : acc 0.772758126\n",
            "step  47 : loss 0.333851218 : acc 0.775265932\n",
            "step  48 : loss 0.567162752 : acc 0.77734375\n",
            "step  49 : loss 0.455620497 : acc 0.778858423\n",
            "step  50 : loss 0.348600149 : acc 0.78125\n",
            "step  51 : loss 0.310768843 : acc 0.783701\n",
            "step  52 : loss 0.371136338 : acc 0.785306513\n",
            "step  53 : loss 0.267588735 : acc 0.788178086\n",
            "step  54 : loss 0.284277022 : acc 0.790075243\n",
            "step  55 : loss 0.337495 : acc 0.792187512\n",
            "step  56 : loss 0.395766228 : acc 0.793247759\n",
            "step  57 : loss 0.262877 : acc 0.795504391\n",
            "step  58 : loss 0.495536953 : acc 0.796605587\n",
            "step  59 : loss 0.266093552 : acc 0.798596382\n",
            "step  60 : loss 0.390064418 : acc 0.800260425\n",
            "step  61 : loss 0.21465297 : acc 0.802638292\n",
            "step  62 : loss 0.317396343 : acc 0.804309487\n",
            "step  63 : loss 0.272511035 : acc 0.805927575\n",
            "step  64 : loss 0.419862926 : acc 0.806640625\n",
            "step  65 : loss 0.200559586 : acc 0.808894217\n",
            "step  66 : loss 0.210730955 : acc 0.810842812\n",
            "step  67 : loss 0.207231358 : acc 0.81284982\n",
            "step  68 : loss 0.246088117 : acc 0.814338207\n",
            "step  69 : loss 0.277595103 : acc 0.815896749\n",
            "step  70 : loss 0.317172498 : acc 0.816852689\n",
            "step  71 : loss 0.20307605 : acc 0.818441927\n",
            "step  72 : loss 0.267311037 : acc 0.819552958\n",
            "step  73 : loss 0.29811722 : acc 0.820954621\n",
            "step  74 : loss 0.204331338 : acc 0.822635114\n",
            "step  75 : loss 0.242537558 : acc 0.824062526\n",
            "step  76 : loss 0.288541466 : acc 0.825246692\n",
            "step  77 : loss 0.231569096 : acc 0.826501608\n",
            "step  78 : loss 0.422388911 : acc 0.82652241\n",
            "step  79 : loss 0.19508937 : acc 0.828026116\n",
            "step  80 : loss 0.254135162 : acc 0.82900393\n",
            "step  81 : loss 0.349336118 : acc 0.829764664\n",
            "step  82 : loss 0.378747046 : acc 0.830411613\n",
            "step  83 : loss 0.221239269 : acc 0.831701815\n",
            "step  84 : loss 0.221667022 : acc 0.832868278\n",
            "step  85 : loss 0.314950973 : acc 0.83327204\n",
            "step  86 : loss 0.339105606 : acc 0.834120631\n",
            "step  87 : loss 0.341477394 : acc 0.834949732\n",
            "step  88 : loss 0.312122941 : acc 0.835671186\n",
            "step  89 : loss 0.240832329 : acc 0.836551964\n",
            "step  90 : loss 0.269152164 : acc 0.837413192\n",
            "step  91 : loss 0.231321245 : acc 0.838513076\n",
            "step  92 : loss 0.187643975 : acc 0.839673936\n",
            "step  93 : loss 0.15739 : acc 0.840977848\n",
            "step  94 : loss 0.310136735 : acc 0.841506\n",
            "step  95 : loss 0.170982748 : acc 0.842680931\n",
            "step  96 : loss 0.289688587 : acc 0.843180358\n",
            "step  97 : loss 0.295530498 : acc 0.843911111\n",
            "step  98 : loss 0.342613161 : acc 0.844467461\n",
            "step  99 : loss 0.297312737 : acc 0.845170438\n",
            "step  100 : loss 0.161265612 : acc 0.84625\n",
            "step  101 : loss 0.372482806 : acc 0.846612\n",
            "step  102 : loss 0.250743508 : acc 0.847349882\n",
            "step  103 : loss 0.294288903 : acc 0.8481493\n",
            "step  104 : loss 0.338989735 : acc 0.848707914\n",
            "step  105 : loss 0.190796524 : acc 0.849628\n",
            "step  106 : loss 0.246582612 : acc 0.850383282\n",
            "step  107 : loss 0.201343298 : acc 0.851343453\n",
            "step  108 : loss 0.200787812 : acc 0.852285862\n",
            "step  109 : loss 0.150484234 : acc 0.853139341\n",
            "step  110 : loss 0.299436837 : acc 0.853551149\n",
            "step  111 : loss 0.196617916 : acc 0.854237\n",
            "step  112 : loss 0.131611377 : acc 0.855399\n",
            "step  113 : loss 0.33679235 : acc 0.855918169\n",
            "step  114 : loss 0.274911046 : acc 0.856770813\n",
            "step  115 : loss 0.206790522 : acc 0.857676625\n",
            "step  116 : loss 0.167706072 : acc 0.858364761\n",
            "step  117 : loss 0.359685242 : acc 0.858707249\n",
            "step  118 : loss 0.293891549 : acc 0.859242558\n",
            "step  119 : loss 0.139862835 : acc 0.859900236\n",
            "step  120 : loss 0.191743791 : acc 0.860612\n",
            "step  121 : loss 0.212281987 : acc 0.861182868\n",
            "step  122 : loss 0.104982063 : acc 0.862128615\n",
            "step  123 : loss 0.292793423 : acc 0.862487316\n",
            "step  124 : loss 0.14068462 : acc 0.86328125\n",
            "step  125 : loss 0.311863542 : acc 0.8635\n",
            "step  126 : loss 0.129642025 : acc 0.864397347\n",
            "step  127 : loss 0.235168815 : acc 0.864849925\n",
            "step  128 : loss 0.257044673 : acc 0.865356445\n",
            "step  129 : loss 0.214217424 : acc 0.865855157\n",
            "step  130 : loss 0.216418549 : acc 0.866286039\n",
            "step  131 : loss 0.119083859 : acc 0.867068231\n",
            "step  132 : loss 0.214801937 : acc 0.867720187\n",
            "step  133 : loss 0.222477674 : acc 0.868244827\n",
            "step  134 : loss 0.188124701 : acc 0.868878245\n",
            "step  135 : loss 0.231924921 : acc 0.869328678\n",
            "step  136 : loss 0.231983781 : acc 0.869829953\n",
            "step  137 : loss 0.199418515 : acc 0.870380938\n",
            "step  138 : loss 0.151890233 : acc 0.870923936\n",
            "step  139 : loss 0.215511113 : acc 0.871346653\n",
            "step  140 : loss 0.195404276 : acc 0.871819198\n",
            "step  141 : loss 0.201173902 : acc 0.872395813\n",
            "step  142 : loss 0.185364842 : acc 0.872854292\n",
            "step  143 : loss 0.198658317 : acc 0.873361\n",
            "step  144 : loss 0.137482956 : acc 0.873914957\n",
            "step  145 : loss 0.182559788 : acc 0.874461234\n",
            "step  146 : loss 0.146614313 : acc 0.875053525\n",
            "step  147 : loss 0.140183419 : acc 0.875690877\n",
            "step  148 : loss 0.11303854 : acc 0.876425266\n",
            "step  149 : loss 0.128033817 : acc 0.876992464\n",
            "step  150 : loss 0.189010426 : acc 0.8775\n",
            "step  151 : loss 0.0731752887 : acc 0.878207803\n",
            "step  152 : loss 0.0937774 : acc 0.878854871\n",
            "step  153 : loss 0.177269563 : acc 0.879187107\n",
            "step  154 : loss 0.181802094 : acc 0.879515\n",
            "step  155 : loss 0.2190575 : acc 0.879939497\n",
            "step  156 : loss 0.108505681 : acc 0.880458713\n",
            "step  157 : loss 0.130442262 : acc 0.881070852\n",
            "step  158 : loss 0.169772074 : acc 0.881477475\n",
            "step  159 : loss 0.181978643 : acc 0.88173151\n",
            "step  160 : loss 0.137427703 : acc 0.882324219\n",
            "step  161 : loss 0.141708165 : acc 0.882764\n",
            "step  162 : loss 0.152869031 : acc 0.883150101\n",
            "step  163 : loss 0.113061152 : acc 0.88372314\n",
            "step  164 : loss 0.0964320228 : acc 0.884193957\n",
            "step  165 : loss 0.220408738 : acc 0.884517074\n",
            "step  166 : loss 0.0828707293 : acc 0.885118604\n",
            "step  167 : loss 0.147562563 : acc 0.885666192\n",
            "step  168 : loss 0.173082173 : acc 0.885974705\n",
            "step  169 : loss 0.119598031 : acc 0.886464477\n",
            "step  170 : loss 0.173285246 : acc 0.886764705\n",
            "step  171 : loss 0.182258368 : acc 0.887061417\n",
            "step  172 : loss 0.274389565 : acc 0.887218416\n",
            "step  173 : loss 0.23728931 : acc 0.887463868\n",
            "step  174 : loss 0.284400702 : acc 0.887706518\n",
            "step  175 : loss 0.279203326 : acc 0.887946427\n",
            "step  176 : loss 0.146248475 : acc 0.888272345\n",
            "step  177 : loss 0.117309272 : acc 0.888727069\n",
            "step  178 : loss 0.143528491 : acc 0.889176607\n",
            "step  179 : loss 0.136662275 : acc 0.889577508\n",
            "step  180 : loss 0.105342418 : acc 0.89001739\n",
            "step  181 : loss 0.224252373 : acc 0.890236557\n",
            "step  182 : loss 0.195422605 : acc 0.890582085\n",
            "step  183 : loss 0.144092903 : acc 0.891009212\n",
            "step  184 : loss 0.117057279 : acc 0.891431749\n",
            "step  185 : loss 0.196187973 : acc 0.891638517\n",
            "step  186 : loss 0.176527038 : acc 0.891969085\n",
            "step  187 : loss 0.117846839 : acc 0.892421484\n",
            "step  188 : loss 0.149734929 : acc 0.892869\n",
            "step  189 : loss 0.147858679 : acc 0.893229187\n",
            "step  190 : loss 0.110923037 : acc 0.893503308\n",
            "step  191 : loss 0.20140475 : acc 0.893774569\n",
            "step  192 : loss 0.122448772 : acc 0.894165039\n",
            "step  193 : loss 0.175705895 : acc 0.89438957\n",
            "step  194 : loss 0.188466311 : acc 0.89453125\n",
            "step  195 : loss 0.145872727 : acc 0.894871771\n",
            "step  196 : loss 0.151351988 : acc 0.895208836\n",
            "step  197 : loss 0.101694793 : acc 0.895502865\n",
            "step  198 : loss 0.17133069 : acc 0.895754397\n",
            "step  199 : loss 0.174209833 : acc 0.895964205\n",
            "step  200 : loss 0.115380794 : acc 0.896171868\n",
            "step  201 : loss 0.206066757 : acc 0.896416366\n",
            "step  202 : loss 0.119744033 : acc 0.896774471\n",
            "step  203 : loss 0.165215939 : acc 0.897013545\n",
            "step  204 : loss 0.176971048 : acc 0.897173703\n",
            "step  205 : loss 0.128818154 : acc 0.897484779\n",
            "step  206 : loss 0.162261963 : acc 0.897830725\n",
            "step  207 : loss 0.101922467 : acc 0.898060083\n",
            "step  208 : loss 0.210044622 : acc 0.898137033\n",
            "step  209 : loss 0.12617591 : acc 0.898474872\n",
            "step  210 : loss 0.147256374 : acc 0.898772299\n",
            "step  211 : loss 0.177210584 : acc 0.89910394\n",
            "step  212 : loss 0.221320048 : acc 0.899211407\n",
            "step  213 : loss 0.168345258 : acc 0.899537861\n",
            "step  214 : loss 0.154632106 : acc 0.899824739\n",
            "step  215 : loss 0.175961941 : acc 0.900109\n",
            "step  216 : loss 0.208409 : acc 0.900282145\n",
            "step  217 : loss 0.120245159 : acc 0.90052563\n",
            "step  218 : loss 0.275832236 : acc 0.900516033\n",
            "step  219 : loss 0.117058456 : acc 0.900756299\n",
            "step  220 : loss 0.217605785 : acc 0.900958836\n",
            "step  221 : loss 0.142088681 : acc 0.901230216\n",
            "step  222 : loss 0.176398635 : acc 0.901288033\n",
            "step  223 : loss 0.10387674 : acc 0.901555479\n",
            "step  224 : loss 0.114374846 : acc 0.901890337\n",
            "step  225 : loss 0.182522148 : acc 0.902048588\n",
            "step  226 : loss 0.259504855 : acc 0.902170897\n",
            "step  227 : loss 0.1210225 : acc 0.902361\n",
            "step  228 : loss 0.171057254 : acc 0.902549326\n",
            "step  229 : loss 0.149208859 : acc 0.902736068\n",
            "step  230 : loss 0.188313603 : acc 0.902887225\n",
            "step  231 : loss 0.209589243 : acc 0.90296942\n",
            "step  232 : loss 0.148526415 : acc 0.903185606\n",
            "step  233 : loss 0.124175817 : acc 0.903433502\n",
            "step  234 : loss 0.225232542 : acc 0.903545678\n",
            "step  235 : loss 0.149577498 : acc 0.903723419\n",
            "step  236 : loss 0.174304232 : acc 0.903998911\n",
            "step  237 : loss 0.163116068 : acc 0.904107332\n",
            "step  238 : loss 0.144444197 : acc 0.904181957\n",
            "step  239 : loss 0.164450869 : acc 0.904452145\n",
            "step  240 : loss 0.119887993 : acc 0.904622376\n",
            "step  241 : loss 0.134555265 : acc 0.904888511\n",
            "step  242 : loss 0.140172869 : acc 0.905120075\n",
            "step  243 : loss 0.16212678 : acc 0.905253351\n",
            "step  244 : loss 0.127589434 : acc 0.905481577\n",
            "step  245 : loss 0.096487157 : acc 0.905771673\n",
            "step  246 : loss 0.1120698 : acc 0.905995905\n",
            "step  247 : loss 0.126639664 : acc 0.90621835\n",
            "step  248 : loss 0.238293827 : acc 0.906376\n",
            "step  249 : loss 0.157423481 : acc 0.906563759\n",
            "step  250 : loss 0.0630454868 : acc 0.906843722\n",
            "step  251 : loss 0.0759666264 : acc 0.907121539\n",
            "step  252 : loss 0.0676627457 : acc 0.907490075\n",
            "step  253 : loss 0.11864917 : acc 0.907732189\n",
            "step  254 : loss 0.187698394 : acc 0.907849431\n",
            "step  255 : loss 0.113609605 : acc 0.908088207\n",
            "step  256 : loss 0.183951765 : acc 0.908233643\n",
            "step  257 : loss 0.144554496 : acc 0.908438742\n",
            "step  258 : loss 0.191558242 : acc 0.908490777\n",
            "step  259 : loss 0.0842586532 : acc 0.908753633\n",
            "step  260 : loss 0.0575084276 : acc 0.909074545\n",
            "step  261 : loss 0.0934865773 : acc 0.909303188\n",
            "step  262 : loss 0.0860839859 : acc 0.909559906\n",
            "step  263 : loss 0.0694861412 : acc 0.909874\n",
            "step  264 : loss 0.162147969 : acc 0.910008311\n",
            "step  265 : loss 0.17122677 : acc 0.910200477\n",
            "step  266 : loss 0.15057078 : acc 0.910391212\n",
            "step  267 : loss 0.0745652169 : acc 0.910639048\n",
            "step  268 : loss 0.125766724 : acc 0.910826743\n",
            "step  269 : loss 0.0618948154 : acc 0.911129177\n",
            "step  270 : loss 0.202148035 : acc 0.911342621\n",
            "step  271 : loss 0.0630309805 : acc 0.911612093\n",
            "step  272 : loss 0.156418979 : acc 0.911764681\n",
            "step  273 : loss 0.172811091 : acc 0.911887586\n",
            "step  274 : loss 0.159707576 : acc 0.912066579\n",
            "step  275 : loss 0.0799061507 : acc 0.912301123\n",
            "step  276 : loss 0.192909732 : acc 0.912477374\n",
            "step  277 : loss 0.153943658 : acc 0.912624121\n",
            "step  278 : loss 0.243537769 : acc 0.912741661\n",
            "step  279 : loss 0.112169318 : acc 0.91294241\n",
            "step  280 : loss 0.101085536 : acc 0.913169622\n",
            "step  281 : loss 0.0726889223 : acc 0.913395226\n",
            "step  282 : loss 0.096154049 : acc 0.913563848\n",
            "step  283 : loss 0.0628501251 : acc 0.913814068\n",
            "step  284 : loss 0.0902515352 : acc 0.914007485\n",
            "step  285 : loss 0.0923079699 : acc 0.914226949\n",
            "step  286 : loss 0.203121141 : acc 0.914335668\n",
            "step  287 : loss 0.0741153061 : acc 0.91457969\n",
            "step  288 : loss 0.171816021 : acc 0.914686441\n",
            "step  289 : loss 0.129320115 : acc 0.914900541\n",
            "step  290 : loss 0.114406064 : acc 0.915032327\n",
            "step  291 : loss 0.10661906 : acc 0.915243745\n",
            "step  292 : loss 0.139214873 : acc 0.915293217\n",
            "step  293 : loss 0.0575096905 : acc 0.915529\n",
            "step  294 : loss 0.114431746 : acc 0.915710032\n",
            "step  295 : loss 0.114142381 : acc 0.915889859\n",
            "step  296 : loss 0.197605729 : acc 0.91598922\n",
            "step  297 : loss 0.0469354391 : acc 0.916245818\n",
            "step  298 : loss 0.0897300392 : acc 0.916448176\n",
            "step  299 : loss 0.0868515819 : acc 0.916597\n",
            "step  300 : loss 0.0685115829 : acc 0.91682291\n",
            "step  301 : loss 0.321109116 : acc 0.916813731\n",
            "step  302 : loss 0.122635394 : acc 0.917011619\n",
            "step  303 : loss 0.186833099 : acc 0.917105\n",
            "step  304 : loss 0.126478732 : acc 0.917249203\n",
            "step  305 : loss 0.186753437 : acc 0.917315602\n",
            "step  306 : loss 0.15280652 : acc 0.917432606\n",
            "step  307 : loss 0.141414881 : acc 0.917548835\n",
            "step  308 : loss 0.125026077 : acc 0.917664349\n",
            "step  309 : loss 0.0774415806 : acc 0.917829692\n",
            "step  310 : loss 0.114065006 : acc 0.918019176\n",
            "step  311 : loss 0.12415956 : acc 0.918132\n",
            "step  312 : loss 0.0439422168 : acc 0.918369412\n",
            "step  313 : loss 0.126419455 : acc 0.918505371\n",
            "step  314 : loss 0.0768796206 : acc 0.918715179\n",
            "step  315 : loss 0.0852482915 : acc 0.91884923\n",
            "step  316 : loss 0.061869517 : acc 0.919056594\n",
            "step  317 : loss 0.121925145 : acc 0.919164062\n",
            "step  318 : loss 0.118711784 : acc 0.91932\n",
            "step  319 : loss 0.113429546 : acc 0.9194749\n",
            "step  320 : loss 0.125726879 : acc 0.919580102\n",
            "step  321 : loss 0.0411454774 : acc 0.919806242\n",
            "step  322 : loss 0.145627737 : acc 0.919861197\n",
            "step  323 : loss 0.118957885 : acc 0.919988394\n",
            "step  324 : loss 0.0893595666 : acc 0.920138896\n",
            "step  325 : loss 0.106824897 : acc 0.920264423\n",
            "step  326 : loss 0.0856846273 : acc 0.920461059\n",
            "step  327 : loss 0.0778652281 : acc 0.920608759\n",
            "step  328 : loss 0.0592454486 : acc 0.920803189\n",
            "step  329 : loss 0.131992176 : acc 0.92092514\n",
            "step  330 : loss 0.203613698 : acc 0.920975387\n",
            "step  331 : loss 0.103925407 : acc 0.921143293\n",
            "step  332 : loss 0.0914115384 : acc 0.921286702\n",
            "step  333 : loss 0.0926855803 : acc 0.921429217\n",
            "step  334 : loss 0.0662452728 : acc 0.921617687\n",
            "step  335 : loss 0.0714201927 : acc 0.921805\n",
            "step  336 : loss 0.11716263 : acc 0.921921492\n",
            "step  337 : loss 0.11113368 : acc 0.92206049\n",
            "step  338 : loss 0.0832571238 : acc 0.92222172\n",
            "step  339 : loss 0.138214648 : acc 0.922312856\n",
            "step  340 : loss 0.0626524761 : acc 0.922518373\n",
            "step  341 : loss 0.0721018538 : acc 0.922676861\n",
            "step  342 : loss 0.0745362788 : acc 0.922811568\n",
            "step  343 : loss 0.0862619728 : acc 0.922968268\n",
            "step  344 : loss 0.0837455466 : acc 0.923146784\n",
            "step  345 : loss 0.101752788 : acc 0.923279\n",
            "step  346 : loss 0.162421554 : acc 0.923433\n",
            "step  347 : loss 0.0960174 : acc 0.923608601\n",
            "step  348 : loss 0.211170554 : acc 0.923648536\n",
            "step  349 : loss 0.0584658757 : acc 0.923777759\n",
            "step  350 : loss 0.0537884906 : acc 0.923995554\n",
            "step  351 : loss 0.162599429 : acc 0.924056292\n",
            "step  352 : loss 0.24088791 : acc 0.924116671\n",
            "step  353 : loss 0.0895710737 : acc 0.924221\n",
            "step  354 : loss 0.178245634 : acc 0.924258471\n",
            "step  355 : loss 0.0791161656 : acc 0.924405813\n",
            "step  356 : loss 0.0759493634 : acc 0.924574256\n",
            "step  357 : loss 0.0819719434 : acc 0.92471987\n",
            "step  358 : loss 0.0937917233 : acc 0.924842894\n",
            "step  359 : loss 0.123948932 : acc 0.924921632\n",
            "step  360 : loss 0.0977016315 : acc 0.925043404\n",
            "step  361 : loss 0.1160319 : acc 0.925099552\n",
            "step  362 : loss 0.0984450877 : acc 0.925198555\n",
            "step  363 : loss 0.133809775 : acc 0.925275505\n",
            "step  364 : loss 0.199333787 : acc 0.92533052\n",
            "step  365 : loss 0.159803867 : acc 0.925449491\n",
            "step  366 : loss 0.11102552 : acc 0.925610483\n",
            "step  367 : loss 0.117603377 : acc 0.925706744\n",
            "step  368 : loss 0.0964532793 : acc 0.925866187\n",
            "step  369 : loss 0.156895056 : acc 0.925982356\n",
            "step  370 : loss 0.156943321 : acc 0.926055729\n",
            "step  371 : loss 0.143725857 : acc 0.926149786\n",
            "step  372 : loss 0.0524890199 : acc 0.926306307\n",
            "step  373 : loss 0.153625667 : acc 0.926441\n",
            "step  374 : loss 0.126076818 : acc 0.926533282\n",
            "step  375 : loss 0.0667706132 : acc 0.926666677\n",
            "step  376 : loss 0.104327425 : acc 0.926716268\n",
            "step  377 : loss 0.0979229733 : acc 0.926827729\n",
            "step  378 : loss 0.0533281825 : acc 0.92698\n",
            "step  379 : loss 0.125318915 : acc 0.927069604\n",
            "step  380 : loss 0.157440484 : acc 0.927117586\n",
            "step  381 : loss 0.0767460689 : acc 0.927267909\n",
            "step  382 : loss 0.0625919476 : acc 0.927396953\n",
            "step  383 : loss 0.0621119961 : acc 0.927525282\n",
            "step  384 : loss 0.144999087 : acc 0.92763263\n",
            "step  385 : loss 0.0784351379 : acc 0.927739441\n",
            "step  386 : loss 0.111265898 : acc 0.927825451\n",
            "step  387 : loss 0.0355757587 : acc 0.927991748\n",
            "step  388 : loss 0.10551919 : acc 0.928116918\n",
            "step  389 : loss 0.048114188 : acc 0.928261578\n",
            "step  390 : loss 0.0931133 : acc 0.928385437\n",
            "step  391 : loss 0.148163557 : acc 0.928468645\n",
            "step  392 : loss 0.13301003 : acc 0.928531587\n",
            "step  393 : loss 0.116335496 : acc 0.928653777\n",
            "step  394 : loss 0.108718574 : acc 0.928755581\n",
            "step  395 : loss 0.115286469 : acc 0.928797483\n",
            "step  396 : loss 0.103604384 : acc 0.928918064\n",
            "step  397 : loss 0.115242928 : acc 0.928998768\n",
            "step  398 : loss 0.0597738475 : acc 0.929157495\n",
            "step  399 : loss 0.0695859343 : acc 0.929256737\n",
            "step  400 : loss 0.140727714 : acc 0.929335952\n",
            "step  401 : loss 0.089223966 : acc 0.929453731\n",
            "step  402 : loss 0.0565512329 : acc 0.929570913\n",
            "step  403 : loss 0.0853443742 : acc 0.929668128\n",
            "step  404 : loss 0.0981372297 : acc 0.92980355\n",
            "step  405 : loss 0.157504037 : acc 0.929919\n",
            "step  406 : loss 0.0869183838 : acc 0.930033863\n",
            "step  407 : loss 0.113096535 : acc 0.930148184\n",
            "step  408 : loss 0.221340671 : acc 0.930185378\n",
            "step  409 : loss 0.140145227 : acc 0.930298746\n",
            "step  410 : loss 0.0781584606 : acc 0.930449724\n",
            "step  411 : loss 0.10079921 : acc 0.930485845\n",
            "step  412 : loss 0.238716483 : acc 0.930502892\n",
            "step  413 : loss 0.0760522932 : acc 0.930576563\n",
            "step  414 : loss 0.191870824 : acc 0.930593312\n",
            "step  415 : loss 0.0517357327 : acc 0.930722892\n",
            "step  416 : loss 0.122234732 : acc 0.930776715\n",
            "step  417 : loss 0.0677705482 : acc 0.930886567\n",
            "step  418 : loss 0.0920597911 : acc 0.930977106\n",
            "step  419 : loss 0.0881195292 : acc 0.931067288\n",
            "step  420 : loss 0.136951715 : acc 0.931138396\n",
            "step  421 : loss 0.0983361527 : acc 0.931227744\n",
            "step  422 : loss 0.0714282542 : acc 0.931335151\n",
            "step  423 : loss 0.108732566 : acc 0.931423604\n",
            "step  424 : loss 0.107452728 : acc 0.931474805\n",
            "step  425 : loss 0.111509137 : acc 0.931599259\n",
            "step  426 : loss 0.0471058413 : acc 0.931741476\n",
            "step  427 : loss 0.0556187257 : acc 0.931864738\n",
            "step  428 : loss 0.109642558 : acc 0.931932688\n",
            "step  429 : loss 0.0849210471 : acc 0.932036698\n",
            "step  430 : loss 0.0564321019 : acc 0.932194769\n",
            "step  431 : loss 0.0621511713 : acc 0.932315826\n",
            "step  432 : loss 0.0299764518 : acc 0.932472527\n",
            "step  433 : loss 0.0883435458 : acc 0.932538271\n",
            "step  434 : loss 0.0928146765 : acc 0.932657719\n",
            "step  435 : loss 0.160079 : acc 0.932722688\n",
            "step  436 : loss 0.0863834098 : acc 0.932823241\n",
            "step  437 : loss 0.0228320602 : acc 0.932976961\n",
            "step  438 : loss 0.0341368653 : acc 0.933129966\n",
            "step  439 : loss 0.0874689 : acc 0.933211148\n",
            "step  440 : loss 0.0513903201 : acc 0.933327436\n",
            "step  441 : loss 0.0196950436 : acc 0.933478594\n",
            "step  442 : loss 0.0898034722 : acc 0.933576047\n",
            "step  443 : loss 0.0627996325 : acc 0.9336555\n",
            "step  444 : loss 0.034522526 : acc 0.93380487\n",
            "step  445 : loss 0.136595353 : acc 0.933901\n",
            "step  446 : loss 0.0413358696 : acc 0.934014142\n",
            "step  447 : loss 0.119865224 : acc 0.934074402\n",
            "step  448 : loss 0.0932278186 : acc 0.934134364\n",
            "step  449 : loss 0.0970031917 : acc 0.934228837\n",
            "step  450 : loss 0.0961440206 : acc 0.934288204\n",
            "step  451 : loss 0.0901155919 : acc 0.934347272\n",
            "step  452 : loss 0.047806792 : acc 0.934475243\n",
            "step  453 : loss 0.0535111092 : acc 0.934585392\n",
            "step  454 : loss 0.161490291 : acc 0.934626222\n",
            "step  455 : loss 0.0562892929 : acc 0.934735596\n",
            "step  456 : loss 0.0464151651 : acc 0.934827328\n",
            "step  457 : loss 0.0725814924 : acc 0.934935749\n",
            "step  458 : loss 0.0933688357 : acc 0.935026586\n",
            "step  459 : loss 0.0348740593 : acc 0.935168147\n",
            "step  460 : loss 0.0615899935 : acc 0.935292125\n",
            "step  461 : loss 0.0845134705 : acc 0.935381651\n",
            "step  462 : loss 0.167989373 : acc 0.93538624\n",
            "step  463 : loss 0.11847081 : acc 0.935492039\n",
            "step  464 : loss 0.120900407 : acc 0.935513198\n",
            "step  465 : loss 0.109465063 : acc 0.935567856\n",
            "step  466 : loss 0.0954836309 : acc 0.935655832\n",
            "step  467 : loss 0.164499 : acc 0.935693264\n",
            "step  468 : loss 0.0591898337 : acc 0.935763896\n",
            "step  469 : loss 0.0720214 : acc 0.93585\n",
            "val_loss 0.050688047 : val_ acc 0.9745\n",
            "step  470 : loss 0.0810704529 : acc 0.935936689\n",
            "step  471 : loss 0.050538376 : acc 0.936039567\n",
            "step  472 : loss 0.0782601833 : acc 0.936125457\n",
            "step  473 : loss 0.0701011 : acc 0.936194479\n",
            "step  474 : loss 0.0524881259 : acc 0.936296165\n",
            "step  475 : loss 0.0887758285 : acc 0.936364532\n",
            "step  476 : loss 0.100033164 : acc 0.9364326\n",
            "step  477 : loss 0.0426243246 : acc 0.936549544\n",
            "step  478 : loss 0.100186698 : acc 0.936616957\n",
            "step  479 : loss 0.138212323 : acc 0.93666774\n",
            "step  480 : loss 0.105094209 : acc 0.936734617\n",
            "step  481 : loss 0.0710781 : acc 0.936833739\n",
            "step  482 : loss 0.0621805042 : acc 0.936932385\n",
            "step  483 : loss 0.0795228183 : acc 0.93701452\n",
            "step  484 : loss 0.109053947 : acc 0.937080085\n",
            "step  485 : loss 0.179158658 : acc 0.937129319\n",
            "step  486 : loss 0.116313115 : acc 0.937178314\n",
            "step  487 : loss 0.103291847 : acc 0.937243223\n",
            "step  488 : loss 0.142551512 : acc 0.937307775\n",
            "step  489 : loss 0.077222608 : acc 0.937372148\n",
            "step  490 : loss 0.0272176899 : acc 0.9375\n",
            "step  491 : loss 0.103396155 : acc 0.937595546\n",
            "step  492 : loss 0.0657967478 : acc 0.937690675\n",
            "step  493 : loss 0.126925454 : acc 0.937721968\n",
            "step  494 : loss 0.0792991742 : acc 0.937800646\n",
            "step  495 : loss 0.106183186 : acc 0.937863171\n",
            "step  496 : loss 0.0350309424 : acc 0.937972784\n",
            "step  497 : loss 0.0595024973 : acc 0.938066185\n",
            "step  498 : loss 0.0511465408 : acc 0.938159227\n",
            "step  499 : loss 0.0838642046 : acc 0.938204885\n",
            "step  500 : loss 0.0968616605 : acc 0.938312888\n",
            "step  501 : loss 0.0479154326 : acc 0.938389301\n",
            "step  502 : loss 0.058244057 : acc 0.938480914\n",
            "step  503 : loss 0.159144908 : acc 0.93851006\n",
            "step  504 : loss 0.124548838 : acc 0.938539088\n",
            "step  505 : loss 0.20406875 : acc 0.938552499\n",
            "step  506 : loss 0.0373629257 : acc 0.938658535\n",
            "step  507 : loss 0.0617455244 : acc 0.938733339\n",
            "step  508 : loss 0.0692242086 : acc 0.938838601\n",
            "step  509 : loss 0.19900465 : acc 0.938897431\n",
            "step  510 : loss 0.0442820564 : acc 0.938971281\n",
            "step  511 : loss 0.18858254 : acc 0.939014316\n",
            "step  512 : loss 0.0657319948 : acc 0.939087689\n",
            "step  513 : loss 0.0657567829 : acc 0.939176\n",
            "step  514 : loss 0.0654807687 : acc 0.9392488\n",
            "step  515 : loss 0.0807676166 : acc 0.939336479\n",
            "step  516 : loss 0.0783963352 : acc 0.939378321\n",
            "step  517 : loss 0.0457019508 : acc 0.939450264\n",
            "step  518 : loss 0.0621195287 : acc 0.939537048\n",
            "step  519 : loss 0.0697408244 : acc 0.939593375\n",
            "step  520 : loss 0.111760631 : acc 0.939634442\n",
            "step  521 : loss 0.117572509 : acc 0.939720333\n",
            "step  522 : loss 0.0981114358 : acc 0.939761043\n",
            "step  523 : loss 0.115036972 : acc 0.939846337\n",
            "step  524 : loss 0.109096453 : acc 0.939901531\n",
            "step  525 : loss 0.0220515914 : acc 0.940016091\n",
            "step  526 : loss 0.0836639181 : acc 0.940100431\n",
            "step  527 : loss 0.0452337861 : acc 0.940199316\n",
            "step  528 : loss 0.137288079 : acc 0.94028306\n",
            "step  529 : loss 0.0419755206 : acc 0.940381229\n",
            "step  530 : loss 0.031215623 : acc 0.940493762\n",
            "step  531 : loss 0.0357653685 : acc 0.940591156\n",
            "step  532 : loss 0.0972129628 : acc 0.940658808\n",
            "step  533 : loss 0.0387437418 : acc 0.940755486\n",
            "step  534 : loss 0.101750463 : acc 0.940778673\n",
            "step  535 : loss 0.0801236257 : acc 0.940860212\n",
            "step  536 : loss 0.0480094068 : acc 0.940941453\n",
            "step  537 : loss 0.0711461 : acc 0.941007793\n",
            "step  538 : loss 0.0500009321 : acc 0.941088438\n",
            "step  539 : loss 0.0513568223 : acc 0.941168785\n",
            "step  540 : loss 0.110364728 : acc 0.941205442\n",
            "step  541 : loss 0.119748324 : acc 0.94124192\n",
            "step  542 : loss 0.0243348517 : acc 0.941350341\n",
            "step  543 : loss 0.0362362936 : acc 0.941429615\n",
            "step  544 : loss 0.11445789 : acc 0.941436768\n",
            "step  545 : loss 0.0706137493 : acc 0.941486895\n",
            "step  546 : loss 0.075995706 : acc 0.941536903\n",
            "step  547 : loss 0.0677453578 : acc 0.941615224\n",
            "step  548 : loss 0.0157591701 : acc 0.941721797\n",
            "step  549 : loss 0.117950678 : acc 0.941785336\n",
            "step  550 : loss 0.099568516 : acc 0.941820145\n",
            "step  551 : loss 0.101172678 : acc 0.941854835\n",
            "step  552 : loss 0.0307276659 : acc 0.941960216\n",
            "step  553 : loss 0.116751201 : acc 0.941980422\n",
            "step  554 : loss 0.0555993095 : acc 0.942042887\n",
            "step  555 : loss 0.022942964 : acc 0.942133307\n",
            "step  556 : loss 0.123054475 : acc 0.94218117\n",
            "step  557 : loss 0.0740748569 : acc 0.942257\n",
            "step  558 : loss 0.120574147 : acc 0.942304432\n",
            "step  559 : loss 0.128621012 : acc 0.942365766\n",
            "step  560 : loss 0.0631468296 : acc 0.942412913\n",
            "step  561 : loss 0.0432573929 : acc 0.942501664\n",
            "step  562 : loss 0.0418419 : acc 0.94257623\n",
            "step  563 : loss 0.101281263 : acc 0.942594945\n",
            "step  564 : loss 0.0415352061 : acc 0.942682922\n",
            "step  565 : loss 0.081739828 : acc 0.942756772\n",
            "step  566 : loss 0.193233967 : acc 0.942761242\n",
            "step  567 : loss 0.0339563154 : acc 0.942862272\n",
            "step  568 : loss 0.0226777978 : acc 0.942962885\n",
            "step  569 : loss 0.0293009467 : acc 0.943049431\n",
            "step  570 : loss 0.0885659 : acc 0.94312197\n",
            "step  571 : loss 0.0751815066 : acc 0.943180561\n",
            "step  572 : loss 0.097172685 : acc 0.943225324\n",
            "step  573 : loss 0.0461155474 : acc 0.943283498\n",
            "step  574 : loss 0.0687876418 : acc 0.943355143\n",
            "step  575 : loss 0.0818497315 : acc 0.94342649\n",
            "step  576 : loss 0.0630650893 : acc 0.943497598\n",
            "step  577 : loss 0.0585173 : acc 0.943568468\n",
            "step  578 : loss 0.0302737169 : acc 0.94365263\n",
            "step  579 : loss 0.0862194747 : acc 0.943723\n",
            "step  580 : loss 0.155958489 : acc 0.943766177\n",
            "step  581 : loss 0.123429619 : acc 0.943809211\n",
            "step  582 : loss 0.053924419 : acc 0.943865478\n",
            "step  583 : loss 0.0537424237 : acc 0.943935\n",
            "step  584 : loss 0.066323474 : acc 0.944004297\n",
            "step  585 : loss 0.104214817 : acc 0.944046617\n",
            "step  586 : loss 0.0393838733 : acc 0.944128811\n",
            "step  587 : loss 0.0573461205 : acc 0.944184065\n",
            "step  588 : loss 0.0658423379 : acc 0.94423914\n",
            "step  589 : loss 0.121928543 : acc 0.944294035\n",
            "step  590 : loss 0.0509249195 : acc 0.944362044\n",
            "step  591 : loss 0.0760111585 : acc 0.944429755\n",
            "step  592 : loss 0.040629141 : acc 0.94451046\n",
            "step  593 : loss 0.15701364 : acc 0.944538176\n",
            "step  594 : loss 0.0690711066 : acc 0.944592118\n",
            "step  595 : loss 0.0751987472 : acc 0.944659\n",
            "step  596 : loss 0.0619520545 : acc 0.944725633\n",
            "step  597 : loss 0.0355600715 : acc 0.944805205\n",
            "step  598 : loss 0.0576934963 : acc 0.944845259\n",
            "step  599 : loss 0.10173697 : acc 0.944872141\n",
            "step  600 : loss 0.051685527 : acc 0.944938\n",
            "step  601 : loss 0.0670588091 : acc 0.945003629\n",
            "step  602 : loss 0.0815710425 : acc 0.945043087\n",
            "step  603 : loss 0.0770765692 : acc 0.94509542\n",
            "step  604 : loss 0.0632163286 : acc 0.945147514\n",
            "step  605 : loss 0.114020042 : acc 0.945186555\n",
            "step  606 : loss 0.0250438433 : acc 0.945264161\n",
            "step  607 : loss 0.0536284633 : acc 0.945315719\n",
            "step  608 : loss 0.0368542038 : acc 0.945392847\n",
            "step  609 : loss 0.0360848159 : acc 0.945469737\n",
            "step  610 : loss 0.0515087768 : acc 0.945520699\n",
            "step  611 : loss 0.0378819853 : acc 0.945597112\n",
            "step  612 : loss 0.120112918 : acc 0.945622206\n",
            "step  613 : loss 0.057530839 : acc 0.945698202\n",
            "step  614 : loss 0.0304395109 : acc 0.945773959\n",
            "step  615 : loss 0.0522373319 : acc 0.945836723\n",
            "step  616 : loss 0.0380731821 : acc 0.945912\n",
            "step  617 : loss 0.0616571531 : acc 0.945961714\n",
            "step  618 : loss 0.0551005974 : acc 0.946023881\n",
            "step  619 : loss 0.0518363602 : acc 0.946098506\n",
            "step  620 : loss 0.0646579787 : acc 0.946147621\n",
            "step  621 : loss 0.0516924448 : acc 0.946221828\n",
            "step  622 : loss 0.0498607792 : acc 0.946270585\n",
            "step  623 : loss 0.0535380952 : acc 0.946331799\n",
            "step  624 : loss 0.0640744939 : acc 0.946380258\n",
            "step  625 : loss 0.0543607175 : acc 0.946428597\n",
            "step  626 : loss 0.117623761 : acc 0.946476758\n",
            "step  627 : loss 0.0192394257 : acc 0.946562111\n",
            "step  628 : loss 0.035479039 : acc 0.946634829\n",
            "step  629 : loss 0.0311647095 : acc 0.946694851\n",
            "step  630 : loss 0.113722861 : acc 0.946717441\n",
            "step  631 : loss 0.0526717864 : acc 0.946764767\n",
            "step  632 : loss 0.0436412692 : acc 0.94683665\n",
            "step  633 : loss 0.0443987958 : acc 0.946908355\n",
            "step  634 : loss 0.0736928359 : acc 0.946955144\n",
            "step  635 : loss 0.088680692 : acc 0.946977139\n",
            "step  636 : loss 0.0681472 : acc 0.947035968\n",
            "step  637 : loss 0.0618289337 : acc 0.947082341\n",
            "step  638 : loss 0.172154024 : acc 0.947104096\n",
            "step  639 : loss 0.135296285 : acc 0.947137952\n",
            "step  640 : loss 0.0224587135 : acc 0.947220623\n",
            "step  641 : loss 0.0774185434 : acc 0.9472664\n",
            "step  642 : loss 0.0978202373 : acc 0.947287679\n",
            "step  643 : loss 0.0919589102 : acc 0.947308898\n",
            "step  644 : loss 0.0257054567 : acc 0.947390795\n",
            "step  645 : loss 0.0247185864 : acc 0.947460234\n",
            "step  646 : loss 0.131799132 : acc 0.947493196\n",
            "step  647 : loss 0.0213320218 : acc 0.947574437\n",
            "step  648 : loss 0.0415665656 : acc 0.94764328\n",
            "step  649 : loss 0.0891513675 : acc 0.947687864\n",
            "step  650 : loss 0.036127232 : acc 0.94775635\n",
            "step  651 : loss 0.0574946105 : acc 0.947800636\n",
            "step  652 : loss 0.0927167833 : acc 0.947844744\n",
            "step  653 : loss 0.0382447205 : acc 0.947912693\n",
            "step  654 : loss 0.0711187348 : acc 0.947968423\n",
            "step  655 : loss 0.0800017565 : acc 0.948012114\n",
            "step  656 : loss 0.0436210819 : acc 0.948055685\n",
            "step  657 : loss 0.0370793045 : acc 0.948122859\n",
            "step  658 : loss 0.0318298861 : acc 0.948189855\n",
            "step  659 : loss 0.0968499929 : acc 0.948209226\n",
            "step  660 : loss 0.131834596 : acc 0.94824034\n",
            "step  661 : loss 0.0269833542 : acc 0.94831866\n",
            "step  662 : loss 0.0882079 : acc 0.948349535\n",
            "step  663 : loss 0.104142942 : acc 0.948368549\n",
            "step  664 : loss 0.0190111622 : acc 0.948446333\n",
            "step  665 : loss 0.0422217026 : acc 0.948500395\n",
            "step  666 : loss 0.0460117348 : acc 0.948554277\n",
            "step  667 : loss 0.0652027428 : acc 0.948608\n",
            "step  668 : loss 0.0633717328 : acc 0.948661566\n",
            "step  669 : loss 0.0920456275 : acc 0.948691607\n",
            "step  670 : loss 0.0714719743 : acc 0.948744893\n",
            "step  671 : loss 0.0257844776 : acc 0.948821306\n",
            "step  672 : loss 0.0320136622 : acc 0.948885858\n",
            "step  673 : loss 0.0750718713 : acc 0.948938608\n",
            "step  674 : loss 0.0590144433 : acc 0.949002802\n",
            "step  675 : loss 0.098364 : acc 0.949032068\n",
            "step  676 : loss 0.0731144175 : acc 0.949061215\n",
            "step  677 : loss 0.105862498 : acc 0.949090302\n",
            "step  678 : loss 0.0853944346 : acc 0.949119329\n",
            "step  679 : loss 0.0198540073 : acc 0.949194312\n",
            "step  680 : loss 0.0634062663 : acc 0.949246049\n",
            "step  681 : loss 0.0875991 : acc 0.949286163\n",
            "step  682 : loss 0.088212654 : acc 0.949326158\n",
            "step  683 : loss 0.0496384352 : acc 0.949389\n",
            "step  684 : loss 0.0183269959 : acc 0.949451566\n",
            "step  685 : loss 0.0363951027 : acc 0.949514\n",
            "step  686 : loss 0.0349280797 : acc 0.949576199\n",
            "step  687 : loss 0.081297338 : acc 0.949626863\n",
            "step  688 : loss 0.0485657156 : acc 0.949677408\n",
            "step  689 : loss 0.04011086 : acc 0.949727774\n",
            "step  690 : loss 0.230842099 : acc 0.949721396\n",
            "step  691 : loss 0.117907576 : acc 0.949737608\n",
            "step  692 : loss 0.150985986 : acc 0.949742496\n",
            "step  693 : loss 0.0442046896 : acc 0.949781239\n",
            "step  694 : loss 0.059081167 : acc 0.949842334\n",
            "step  695 : loss 0.0525845885 : acc 0.949892044\n",
            "step  696 : loss 0.103025444 : acc 0.949919164\n",
            "step  697 : loss 0.105576552 : acc 0.949946165\n",
            "step  698 : loss 0.110482819 : acc 0.949984312\n",
            "step  699 : loss 0.0693581 : acc 0.95002234\n",
            "step  700 : loss 0.0389587507 : acc 0.9500826\n",
            "step  701 : loss 0.0585906878 : acc 0.950109243\n",
            "step  702 : loss 0.00944617 : acc 0.950180352\n",
            "step  703 : loss 0.13687405 : acc 0.95019567\n",
            "step  704 : loss 0.0489708073 : acc 0.950233102\n",
            "step  705 : loss 0.0553440452 : acc 0.95028156\n",
            "step  706 : loss 0.107545748 : acc 0.9503299\n",
            "step  707 : loss 0.0901550278 : acc 0.95037806\n",
            "step  708 : loss 0.124016464 : acc 0.950370908\n",
            "step  709 : loss 0.105909213 : acc 0.950363755\n",
            "step  710 : loss 0.122095734 : acc 0.950356662\n",
            "step  711 : loss 0.0366467908 : acc 0.950415492\n",
            "step  712 : loss 0.0671135187 : acc 0.950441241\n",
            "step  713 : loss 0.0898085237 : acc 0.950477898\n",
            "step  714 : loss 0.0641694739 : acc 0.950492561\n",
            "step  715 : loss 0.121439561 : acc 0.950518072\n",
            "step  716 : loss 0.0265993327 : acc 0.950587213\n",
            "step  717 : loss 0.0811736956 : acc 0.950623453\n",
            "step  718 : loss 0.0804495 : acc 0.950648725\n",
            "step  719 : loss 0.0166929513 : acc 0.95071739\n",
            "step  720 : loss 0.0782771185 : acc 0.950753272\n",
            "step  721 : loss 0.081204623 : acc 0.950778246\n",
            "step  722 : loss 0.0482910722 : acc 0.950835645\n",
            "step  723 : loss 0.0560528785 : acc 0.950871229\n",
            "step  724 : loss 0.0208260156 : acc 0.95092833\n",
            "step  725 : loss 0.0632875636 : acc 0.950974464\n",
            "step  726 : loss 0.066376254 : acc 0.951031268\n",
            "step  727 : loss 0.0863531381 : acc 0.951044917\n",
            "step  728 : loss 0.0679640844 : acc 0.951079965\n",
            "step  729 : loss 0.0269965827 : acc 0.951147079\n",
            "step  730 : loss 0.0497761443 : acc 0.951192617\n",
            "step  731 : loss 0.0781701207 : acc 0.951216638\n",
            "step  732 : loss 0.175305545 : acc 0.951219261\n",
            "step  733 : loss 0.0937358811 : acc 0.951232493\n",
            "step  734 : loss 0.0751878843 : acc 0.951267064\n",
            "step  735 : loss 0.0399355814 : acc 0.951312125\n",
            "step  736 : loss 0.0706640109 : acc 0.951357\n",
            "step  737 : loss 0.0802870914 : acc 0.95138067\n",
            "step  738 : loss 0.0787276551 : acc 0.951414764\n",
            "step  739 : loss 0.0197294578 : acc 0.951480567\n",
            "step  740 : loss 0.0463817045 : acc 0.951525033\n",
            "step  741 : loss 0.0729006752 : acc 0.951569378\n",
            "step  742 : loss 0.0262069721 : acc 0.951634645\n",
            "step  743 : loss 0.0789550394 : acc 0.951668203\n",
            "step  744 : loss 0.0668704137 : acc 0.951701701\n",
            "step  745 : loss 0.0650690794 : acc 0.95174557\n",
            "step  746 : loss 0.0683663189 : acc 0.951768339\n",
            "step  747 : loss 0.158350557 : acc 0.951791108\n",
            "step  748 : loss 0.0719860941 : acc 0.951824248\n",
            "step  749 : loss 0.120262861 : acc 0.951846838\n",
            "step  750 : loss 0.119326591 : acc 0.951879799\n",
            "step  751 : loss 0.0830630958 : acc 0.951912642\n",
            "step  752 : loss 0.0522021577 : acc 0.951945484\n",
            "step  753 : loss 0.01949629 : acc 0.95200932\n",
            "step  754 : loss 0.0897103548 : acc 0.952052236\n",
            "step  755 : loss 0.0683697686 : acc 0.95208472\n",
            "step  756 : loss 0.0168549549 : acc 0.952148139\n",
            "step  757 : loss 0.0609214939 : acc 0.952180386\n",
            "step  758 : loss 0.0696264803 : acc 0.952212572\n",
            "step  759 : loss 0.0976041853 : acc 0.952254951\n",
            "step  760 : loss 0.0804252774 : acc 0.952266395\n",
            "step  761 : loss 0.0624968112 : acc 0.952308595\n",
            "step  762 : loss 0.0760979503 : acc 0.952330172\n",
            "step  763 : loss 0.0525068194 : acc 0.952341437\n",
            "step  764 : loss 0.050826624 : acc 0.952383399\n",
            "step  765 : loss 0.0387179703 : acc 0.952435434\n",
            "step  766 : loss 0.0634046867 : acc 0.952466965\n",
            "step  767 : loss 0.0378574431 : acc 0.952508569\n",
            "step  768 : loss 0.0520419478 : acc 0.952550054\n",
            "step  769 : loss 0.0167015232 : acc 0.952611804\n",
            "step  770 : loss 0.0746073946 : acc 0.952632725\n",
            "step  771 : loss 0.0220073164 : acc 0.952694237\n",
            "step  772 : loss 0.0450250804 : acc 0.952725112\n",
            "step  773 : loss 0.0448047221 : acc 0.952756\n",
            "step  774 : loss 0.0246188883 : acc 0.952817\n",
            "step  775 : loss 0.057361953 : acc 0.952847719\n",
            "step  776 : loss 0.0784895867 : acc 0.952878237\n",
            "step  777 : loss 0.116118424 : acc 0.952878594\n",
            "step  778 : loss 0.0449428484 : acc 0.952929139\n",
            "step  779 : loss 0.0631531402 : acc 0.952969491\n",
            "step  780 : loss 0.0271225031 : acc 0.953019798\n",
            "step  781 : loss 0.101020195 : acc 0.953029931\n",
            "step  782 : loss 0.0668855086 : acc 0.953070045\n",
            "step  783 : loss 0.00973231718 : acc 0.95313\n",
            "step  784 : loss 0.068596743 : acc 0.953169882\n",
            "step  785 : loss 0.0563956797 : acc 0.953189731\n",
            "step  786 : loss 0.0575374924 : acc 0.953229427\n",
            "step  787 : loss 0.0437207446 : acc 0.953269\n",
            "step  788 : loss 0.0841299221 : acc 0.953298569\n",
            "step  789 : loss 0.0698627681 : acc 0.953318119\n",
            "step  790 : loss 0.0416471809 : acc 0.953357458\n",
            "step  791 : loss 0.135588437 : acc 0.953376949\n",
            "step  792 : loss 0.108616188 : acc 0.95339638\n",
            "step  793 : loss 0.0609642826 : acc 0.953435421\n",
            "step  794 : loss 0.0297684595 : acc 0.953494072\n",
            "step  795 : loss 0.030596327 : acc 0.953542769\n",
            "step  796 : loss 0.0319084637 : acc 0.953601182\n",
            "step  797 : loss 0.095405817 : acc 0.953620195\n",
            "step  798 : loss 0.0170118343 : acc 0.95367831\n",
            "step  799 : loss 0.0818254352 : acc 0.953697205\n",
            "step  800 : loss 0.116982408 : acc 0.953716\n",
            "step  801 : loss 0.065636389 : acc 0.953744531\n",
            "step  802 : loss 0.0422667786 : acc 0.953792512\n",
            "step  803 : loss 0.0310572442 : acc 0.953850031\n",
            "step  804 : loss 0.0470308363 : acc 0.953888\n",
            "step  805 : loss 0.0580147244 : acc 0.953916192\n",
            "step  806 : loss 0.0771096647 : acc 0.953954\n",
            "step  807 : loss 0.0239380151 : acc 0.954011083\n",
            "step  808 : loss 0.0155545 : acc 0.954068\n",
            "step  809 : loss 0.0226888414 : acc 0.954124808\n",
            "step  810 : loss 0.0268060863 : acc 0.954171836\n",
            "step  811 : loss 0.0187395774 : acc 0.954228342\n",
            "step  812 : loss 0.0342322737 : acc 0.954275072\n",
            "step  813 : loss 0.023471741 : acc 0.954321742\n",
            "step  814 : loss 0.0464841276 : acc 0.954349101\n",
            "step  815 : loss 0.0814011544 : acc 0.95437634\n",
            "step  816 : loss 0.0919529572 : acc 0.954413116\n",
            "step  817 : loss 0.0914009511 : acc 0.954421103\n",
            "step  818 : loss 0.0599796697 : acc 0.954438627\n",
            "step  819 : loss 0.082133159 : acc 0.954456091\n",
            "step  820 : loss 0.0310438946 : acc 0.954492629\n",
            "step  821 : loss 0.150543958 : acc 0.954500437\n",
            "step  822 : loss 0.0708412081 : acc 0.954527318\n",
            "step  823 : loss 0.0376126207 : acc 0.954563558\n",
            "step  824 : loss 0.0443014614 : acc 0.954599798\n",
            "step  825 : loss 0.0311272703 : acc 0.954654813\n",
            "step  826 : loss 0.14559184 : acc 0.95467186\n",
            "step  827 : loss 0.0856099278 : acc 0.954679489\n",
            "step  828 : loss 0.11322546 : acc 0.954696476\n",
            "step  829 : loss 0.114457153 : acc 0.954713404\n",
            "step  830 : loss 0.049653098 : acc 0.954758584\n",
            "step  831 : loss 0.0142841246 : acc 0.954813063\n",
            "step  832 : loss 0.109054111 : acc 0.95483917\n",
            "step  833 : loss 0.0472685024 : acc 0.954865277\n",
            "step  834 : loss 0.105242006 : acc 0.954900682\n",
            "step  835 : loss 0.0995867774 : acc 0.954917252\n",
            "step  836 : loss 0.0660248399 : acc 0.954961836\n",
            "step  837 : loss 0.0755923837 : acc 0.954987705\n",
            "step  838 : loss 0.0543793216 : acc 0.955013454\n",
            "step  839 : loss 0.0777032077 : acc 0.955020487\n",
            "step  840 : loss 0.109129965 : acc 0.955008924\n",
            "step  841 : loss 0.0836045668 : acc 0.955025256\n",
            "step  842 : loss 0.0644847602 : acc 0.955060124\n",
            "step  843 : loss 0.046540454 : acc 0.955094934\n",
            "step  844 : loss 0.0219124705 : acc 0.95514816\n",
            "step  845 : loss 0.078266032 : acc 0.955164254\n",
            "step  846 : loss 0.0431140363 : acc 0.955198765\n",
            "step  847 : loss 0.148137033 : acc 0.955214798\n",
            "step  848 : loss 0.0496838316 : acc 0.955230772\n",
            "step  849 : loss 0.0711757392 : acc 0.955255866\n",
            "step  850 : loss 0.0434654392 : acc 0.95528096\n",
            "step  851 : loss 0.0892163217 : acc 0.955296814\n",
            "step  852 : loss 0.0432765707 : acc 0.95531261\n",
            "step  853 : loss 0.0477484316 : acc 0.955328345\n",
            "step  854 : loss 0.0698131 : acc 0.9553532\n",
            "step  855 : loss 0.0506513491 : acc 0.955387175\n",
            "step  856 : loss 0.0803265125 : acc 0.955411911\n",
            "step  857 : loss 0.0226576254 : acc 0.955463946\n",
            "step  858 : loss 0.0902532339 : acc 0.955506802\n",
            "step  859 : loss 0.0575624928 : acc 0.955549479\n",
            "step  860 : loss 0.0997607112 : acc 0.955573916\n",
            "step  861 : loss 0.0514650419 : acc 0.955616474\n",
            "step  862 : loss 0.0747985244 : acc 0.955640793\n",
            "step  863 : loss 0.0507487096 : acc 0.955683112\n",
            "step  864 : loss 0.0373162404 : acc 0.955716372\n",
            "step  865 : loss 0.10665895 : acc 0.955713332\n",
            "step  866 : loss 0.0819350556 : acc 0.955746472\n",
            "step  867 : loss 0.0417542569 : acc 0.955779493\n",
            "step  868 : loss 0.0968247503 : acc 0.955803454\n",
            "step  869 : loss 0.0650376529 : acc 0.955836356\n",
            "step  870 : loss 0.0487978831 : acc 0.955869138\n",
            "step  871 : loss 0.0445130095 : acc 0.955910861\n",
            "step  872 : loss 0.111903824 : acc 0.955934525\n",
            "step  873 : loss 0.0971669555 : acc 0.955958188\n",
            "step  874 : loss 0.0212575793 : acc 0.956008613\n",
            "step  875 : loss 0.0460127741 : acc 0.956032097\n",
            "step  876 : loss 0.0766527355 : acc 0.956064463\n",
            "step  877 : loss 0.0425612219 : acc 0.956096709\n",
            "step  878 : loss 0.126810789 : acc 0.956102252\n",
            "step  879 : loss 0.0564945377 : acc 0.956134439\n",
            "step  880 : loss 0.0978634506 : acc 0.956157625\n",
            "step  881 : loss 0.045413021 : acc 0.956198573\n",
            "step  882 : loss 0.106105179 : acc 0.956212759\n",
            "step  883 : loss 0.0135400835 : acc 0.95626241\n",
            "step  884 : loss 0.0218956172 : acc 0.95630306\n",
            "step  885 : loss 0.121825717 : acc 0.95633477\n",
            "step  886 : loss 0.0821695849 : acc 0.956339955\n",
            "step  887 : loss 0.0455511063 : acc 0.956362784\n",
            "step  888 : loss 0.0291473307 : acc 0.956411898\n",
            "step  889 : loss 0.0256417822 : acc 0.956452191\n",
            "step  890 : loss 0.0906313509 : acc 0.956474781\n",
            "step  891 : loss 0.0585097671 : acc 0.956514895\n",
            "step  892 : loss 0.0612242781 : acc 0.956546128\n",
            "step  893 : loss 0.0797909945 : acc 0.956568539\n",
            "step  894 : loss 0.102016918 : acc 0.956582189\n",
            "step  895 : loss 0.0420649052 : acc 0.956621945\n",
            "step  896 : loss 0.0561864376 : acc 0.956652939\n",
            "step  897 : loss 0.0482464805 : acc 0.95666641\n",
            "step  898 : loss 0.0609659925 : acc 0.956706\n",
            "step  899 : loss 0.0530644283 : acc 0.956736803\n",
            "step  900 : loss 0.0366135426 : acc 0.956767499\n",
            "step  901 : loss 0.0813323483 : acc 0.956789494\n",
            "step  902 : loss 0.0363836512 : acc 0.956828713\n",
            "step  903 : loss 0.0529442579 : acc 0.956850588\n",
            "step  904 : loss 0.0259388816 : acc 0.956889689\n",
            "step  905 : loss 0.0630751625 : acc 0.956911445\n",
            "step  906 : loss 0.0370413065 : acc 0.956941783\n",
            "step  907 : loss 0.0264193062 : acc 0.956980646\n",
            "step  908 : loss 0.122978672 : acc 0.95699358\n",
            "step  909 : loss 0.0318982676 : acc 0.95702374\n",
            "step  910 : loss 0.026445698 : acc 0.957062364\n",
            "step  911 : loss 0.0760693252 : acc 0.957075238\n",
            "step  912 : loss 0.0205640048 : acc 0.957122266\n",
            "step  913 : loss 0.0473239683 : acc 0.957152128\n",
            "step  914 : loss 0.0363374203 : acc 0.957190514\n",
            "step  915 : loss 0.0686463788 : acc 0.957211673\n",
            "step  916 : loss 0.0315314643 : acc 0.957241356\n",
            "step  917 : loss 0.0635471344 : acc 0.957262397\n",
            "step  918 : loss 0.0389022604 : acc 0.957283437\n",
            "step  919 : loss 0.0163912363 : acc 0.957321405\n",
            "step  920 : loss 0.018135678 : acc 0.957367837\n",
            "step  921 : loss 0.0466155559 : acc 0.957397163\n",
            "step  922 : loss 0.0607627854 : acc 0.957426429\n",
            "step  923 : loss 0.0694047138 : acc 0.957464099\n",
            "step  924 : loss 0.0968054 : acc 0.957476318\n",
            "step  925 : loss 0.0575374 : acc 0.957488537\n",
            "step  926 : loss 0.0615573749 : acc 0.957525969\n",
            "step  927 : loss 0.0882253349 : acc 0.957546532\n",
            "step  928 : loss 0.0372058526 : acc 0.95757544\n",
            "step  929 : loss 0.11820668 : acc 0.957595885\n",
            "step  930 : loss 0.0208533 : acc 0.957641482\n",
            "step  931 : loss 0.0254037809 : acc 0.957687\n",
            "step  932 : loss 0.0620664172 : acc 0.957707286\n",
            "step  933 : loss 0.107247964 : acc 0.957727492\n",
            "step  934 : loss 0.0427607819 : acc 0.957764387\n",
            "step  935 : loss 0.0545466766 : acc 0.957784474\n",
            "step  936 : loss 0.0423315801 : acc 0.95782125\n",
            "step  937 : loss 0.0617796965 : acc 0.957849622\n",
            "step  938 : loss 0.0426723212 : acc 0.957866669\n",
            "val_loss 0.0173455309 : val_ acc 0.97755\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDFGSOl8eEuC",
        "outputId": "eebc0e3c-518e-4fa6-a6de-4d8d1ea9d2df"
      },
      "source": [
        "ds_info"
      ],
      "id": "wDFGSOl8eEuC",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tfds.core.DatasetInfo(\n",
              "    name='mnist',\n",
              "    version=3.0.1,\n",
              "    description='The MNIST database of handwritten digits.',\n",
              "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
              "    features=FeaturesDict({\n",
              "        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n",
              "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
              "    }),\n",
              "    total_num_examples=70000,\n",
              "    splits={\n",
              "        'test': 10000,\n",
              "        'train': 60000,\n",
              "    },\n",
              "    supervised_keys=('image', 'label'),\n",
              "    citation=\"\"\"@article{lecun2010mnist,\n",
              "      title={MNIST handwritten digit database},\n",
              "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
              "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
              "      volume={2},\n",
              "      year={2010}\n",
              "    }\"\"\",\n",
              "    redistribution_info=,\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_f7UTIXFjOhb"
      },
      "source": [
        ""
      ],
      "id": "_f7UTIXFjOhb",
      "execution_count": null,
      "outputs": []
    }
  ]
}